{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ed6MpKxq5sOX"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define dataset path (update with actual path on your Google Drive or Colab storage)\n",
        "# dataset_path = '/content/drive/My Drive/kaggle'"
      ],
      "metadata": {
        "id": "FS6iqBub5wOt"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install librosa soundfile torch torchvision torchaudio\n"
      ],
      "metadata": {
        "id": "df-jmYFU6YQC"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOISY DATA GENERATOR"
      ],
      "metadata": {
        "id": "40q-Ad8pZXRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import librosa\n",
        "# import soundfile as sf\n",
        "# import numpy as np\n",
        "# import os\n",
        "\n",
        "# # Add noise to audio\n",
        "# def add_noise(audio, noise_level=0.01):\n",
        "#     noise = np.random.normal(0, noise_level, len(audio))\n",
        "#     return audio + noise\n",
        "\n",
        "# # Create noisy dataset\n",
        "# def create_noisy_dataset(input_folder, output_folder, noise_level=0.01):\n",
        "#     if not os.path.exists(output_folder):\n",
        "#         os.makedirs(output_folder)\n",
        "\n",
        "#     for root, _, files in os.walk(input_folder):\n",
        "#         for file in files:\n",
        "#             if file.endswith(\".wav\"):\n",
        "#                 file_path = os.path.join(root, file)\n",
        "#                 y, sr = librosa.load(file_path, sr=16000)\n",
        "#                 noisy_y = add_noise(y, noise_level)\n",
        "#                 output_path = os.path.join(output_folder, os.path.relpath(file_path, input_folder))\n",
        "#                 os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "#                 sf.write(output_path, noisy_y, sr)\n",
        "\n",
        "# # Example usage\n",
        "# NOISY_DATASET_PATH = '/content/drive/My Drive/kaggle_noisy'\n",
        "# create_noisy_dataset(dataset_path, NOISY_DATASET_PATH, noise_level=0.01)\n"
      ],
      "metadata": {
        "id": "9MQTZRVQ54mv"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN CODE"
      ],
      "metadata": {
        "id": "ltqyR2wXZd7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.functional import pad\n"
      ],
      "metadata": {
        "id": "D65qTE0-Ou06"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(file_path, max_frames=32):\n",
        "    y, sr = librosa.load(file_path, sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    # Pad or truncate to fixed size\n",
        "    if mfcc.shape[1] < max_frames:\n",
        "        mfcc = pad(torch.tensor(mfcc), (0, max_frames - mfcc.shape[1]), mode='constant', value=0)\n",
        "    else:\n",
        "        mfcc = torch.tensor(mfcc[:, :max_frames])\n",
        "    return mfcc\n"
      ],
      "metadata": {
        "id": "nX1g9KlAOwOM"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechCommandDataset(Dataset):\n",
        "    def __init__(self, data_folders, max_frames=32):\n",
        "        self.files = []\n",
        "        self.label_mapping = {\"yes\": 0, \"no\": 1, \"up\": 2, \"down\": 3}  # Adjust based on your dataset\n",
        "\n",
        "        for folder in data_folders:\n",
        "            for root, _, files in os.walk(folder):\n",
        "                for f in files:\n",
        "                    if f.endswith(\".wav\"):\n",
        "                        file_path = os.path.join(root, f)\n",
        "                        label = self.get_label(file_path)\n",
        "                        if label != -1:  # Only include valid files\n",
        "                            self.files.append(file_path)\n",
        "\n",
        "        self.max_frames = max_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.files[idx]\n",
        "        mfcc = extract_features(file_path, max_frames=self.max_frames)\n",
        "        label = self.get_label(file_path)\n",
        "        return mfcc, label\n",
        "\n",
        "    def get_label(self, file_path):\n",
        "        label_name = os.path.basename(os.path.dirname(file_path))  # Assuming folder names are the labels\n",
        "        return self.label_mapping.get(label_name, -1)  # Return -1 if label_name is not in mapping\n"
      ],
      "metadata": {
        "id": "zob5H1J6Oyxo"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    features, labels = zip(*batch)\n",
        "    features_padded = pad_sequence(features, batch_first=True, padding_value=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return features_padded, labels\n"
      ],
      "metadata": {
        "id": "bCr8qnJ8O6ru"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechCommandModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SpeechCommandModel, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # Output shape: (batch_size, 32, height/2, width/2)\n",
        "        )\n",
        "        self.rnn_input_size = 32 * (32 // 2)  # Channels * (Width after max pooling)\n",
        "        self.rnn = nn.LSTM(self.rnn_input_size, 64, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(64 * 2, num_classes)  # Bi-directional LSTM\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN expects a 4D input (batch_size, channels, height, width)\n",
        "        x = self.cnn(x.unsqueeze(1))  # Add channel dimension\n",
        "        x = x.permute(0, 2, 1, 3)  # Reorder to (batch_size, time_steps, channels, features)\n",
        "        x = x.reshape(x.size(0), x.size(1), -1)  # Flatten last two dimensions (batch_size, time_steps, features)\n",
        "        x, _ = self.rnn(x)  # Pass to LSTM\n",
        "        x = self.fc(x[:, -1, :])  # Use the last time step for classification\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "1z2D5OlkO-Lm"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 1, 40, 32)  # Example input: (batch_size, channels, height, width)\n",
        "cnn_output = model.cnn(dummy_input)\n",
        "print(cnn_output.shape)  # Check the shape before reshaping\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KxuzdFeRMeE",
        "outputId": "b9312726-cdf8-49d8-8803-d58c6ec5f39c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32, 20, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, epochs, device):\n",
        "    model = model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for features, labels in dataloader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(features)\n",
        "            loss = loss_fn(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "svfbAdjdPCEe"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels in dataloader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            output = model(features)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "nXebqHvdPFec"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_folders = [\"/content/drive/My Drive/kaggle\", \"/content/drive/My Drive/kaggle_noisy\"]  # Replace with actual paths\n",
        "dataset = SpeechCommandDataset(data_folders=data_folders, max_frames=32)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = SpeechCommandModel(num_classes=4)  # Adjust num_classes based on your labels\n",
        "train_model(model, dataloader, epochs=10, device=device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX_JOl4lPJnO",
        "outputId": "2c0ed5f3-7d54-4742-d9bb-4dea1c205b70"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 5.6506\n",
            "Epoch 2/10, Loss: 0.0495\n",
            "Epoch 3/10, Loss: 0.0239\n",
            "Epoch 4/10, Loss: 0.0146\n",
            "Epoch 5/10, Loss: 0.0100\n",
            "Epoch 6/10, Loss: 0.0074\n",
            "Epoch 7/10, Loss: 0.0057\n",
            "Epoch 8/10, Loss: 0.0045\n",
            "Epoch 9/10, Loss: 0.0037\n",
            "Epoch 10/10, Loss: 0.0031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, dataloader, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7Zw--HFQeBq",
        "outputId": "d6472d2a-a984-4265-fd30-f421b7e478da"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 96.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"speech_command_model.pth\")\n"
      ],
      "metadata": {
        "id": "EENoP-rtXAnj"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SpeechCommandModel(num_classes=4)  # Ensure the architecture matches\n",
        "model.load_state_dict(torch.load(\"speech_command_model.pth\"))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kekjfP9XEVx",
        "outputId": "1ef83e87-2a45-4715-ac76-bc77c5a216a1"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-88-47f3ed9ec621>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"speech_command_model.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SpeechCommandModel(\n",
              "  (cnn): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (rnn): LSTM(512, 64, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=128, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def predict(file_path, model, device):\n",
        "    # Extract features from the input audio file\n",
        "    mfcc = extract_features(file_path).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        output = model(mfcc)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "# Label mapping (same as used during training)\n",
        "label_mapping = {0: \"yes\", 1: \"no\", 2: \"up\", 3: \"down\"}\n"
      ],
      "metadata": {
        "id": "rCCEBpxYXdsw"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example audio file path\n",
        "test_file = \"/content/drive/My Drive/kaggle_noisy/dog/00f0204f_nohash_2.wav\"\n",
        "\n",
        "# Make a prediction\n",
        "predicted_label_index = predict(test_file, model, device)\n",
        "predicted_label = label_mapping[predicted_label_index]\n",
        "\n",
        "print(f\"Predicted Label: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwQCwmSLXfmX",
        "outputId": "199be4ec-5b87-4411-b5c5-b5347fdf7775"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: dog\n"
          ]
        }
      ]
    }
  ]
}